# -*- coding: utf-8 -*-
"""grader.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1H6wUzouQPshK8gSUn-8y3Tz2-aHR4yOb
"""

"""## Idhar se Dekh.

"""
def inp(sample1,sample2):
    global s1
    s1 = sample1
    global s2
    s2 = sample2

    print(s1,s2)

    import nltk
    nltk.download('punkt')

    import nltk
    from nltk.tokenize import word_tokenize, sent_tokenize

    file_docs = []

    with open (s1) as f:
        tokens = sent_tokenize(f.read())
        for line in tokens:
            file_docs.append(line)

    print("Number of documents:",len(file_docs))

    gen_docs = [[w.lower() for w in word_tokenize(text)] 
                for text in file_docs]
    print(gen_docs)

    import gensim
    dictionary = gensim.corpora.Dictionary(gen_docs)
    print(dictionary.token2id)

    corpus = [dictionary.doc2bow(gen_doc) for gen_doc in gen_docs]

    tf_idf = gensim.models.TfidfModel(corpus)
    tf_idf

    import numpy as np
    for doc in tf_idf[corpus]:
        print([[dictionary[id], np.around(freq, decimals=2)] for id, freq in doc])

    # building the index
    sims = gensim.similarities.Similarity('workdir/',tf_idf[corpus],
                                            num_features=len(dictionary))

    sims

    file2_docs = []

    with open (s2) as f:
        tokens = sent_tokenize(f.read())
        for line in tokens:
            file2_docs.append(line)

    print("Number of documents:",len(file2_docs))  
    for line in file2_docs:
        query_doc = [w.lower() for w in word_tokenize(line)]
        query_doc_bow = dictionary.doc2bow(query_doc) #update an existing dictionary and

    # perform a similarity query against the corpus
    query_doc_tf_idf = tf_idf[query_doc_bow]
    # print(document_number, document_similarity)
    print('Comparing Result:', sims[query_doc_tf_idf])

    import numpy as np

    sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))
    print(sum_of_sims)

    percentage_of_similarity = round(float((sum_of_sims / len(file_docs)) * 100))
    print(f'Average similarity float: {float(sum_of_sims / len(file_docs))}')
    print(f'Average similarity percentage: {float(sum_of_sims / len(file_docs)) * 100}')
    print(f'Average similarity rounded percentage: {percentage_of_similarity}')

    avg_sims = [] # array of averages

    # for line in query documents
    for line in file2_docs:
            # tokenize words
            query_doc = [w.lower() for w in word_tokenize(line)]
            # create bag of words
            query_doc_bow = dictionary.doc2bow(query_doc)
            # find similarity for each document
            query_doc_tf_idf = tf_idf[query_doc_bow]
            # print (document_number, document_similarity)
            print('Comparing Result:', sims[query_doc_tf_idf]) 
            # calculate sum of similarities for each query doc
            sum_of_sims =(np.sum(sims[query_doc_tf_idf], dtype=np.float32))
            # calculate average of similarity for each query doc
            avg = sum_of_sims / len(file_docs)
            # print average of similarity for each query doc
            print(f'avg: {sum_of_sims / len(file_docs)}')
            # add average values into array
            avg_sims.append(avg)  
    # calculate total average
    total_avg = np.sum(avg_sims, dtype=np.float)
    # round the value and multiply by 100 to format it as percentage
    percentage_of_similarity = round(float(total_avg) * 100)
    # if percentage is greater than 100
    # that means documents are almost same
    if percentage_of_similarity >= 100:
        percentage_of_similarity = 100
